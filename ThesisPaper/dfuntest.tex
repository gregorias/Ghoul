\chapter{Dfuntest - Streamlined functional testing framework for distributed applications}
\label{ch:dfuntest}
\section{Chapter introduction and overview}
Good practices of software engineering agree \cite{mccon04} that quality code has to be tested and testable.
While many single-application projects rely only on extensive unit-testing frameworks, a distributed application requires more encompassing tools.

Simulation is the most common method to test parallel and distributed algorithms and systems in research.
With right tools (such as SimGrid\cite{cas08}), the simulation environment is realistic, and the whole experiment reproducible.
However, a simulation experiment rarely covers the whole distributed system, from network packets to high level functionalities.
Instead, only a certain, isolated fragment is tested.
Although the simulations confirm the efficiency of an algorithm, the entire application requires a lot more functionalities which are not covered by the simulation, such as the network code, resource management etc.

For the project to be accepted by a wider audience the code needs to be of sufficient quality.
However, performing and reproducing experiments with a distributed application is tedious.
Not only do we have to code the application, but also we need to configure the experimental platform, deploy the application; then, after performing a particular test scenario, gather the results and analyze them.
Developers commonly write ad-hoc scripts to automate many of these tasks; however, these scripts are repetitive, full of boilerplate, and do not add any new core functionality.
Moreover, such scripts are hard to maintain and port between e.g. user credentials, or experimental platforms (e.g. a local cluster or Planet-lab).

We developed dfuntest to test our implementation of Kadmlia, but the project's ambition is to facilitate the process of testing distributed applications in general. 
Dfuntest not only defines a flexible testing pattern, but also automates common tasks such as preparing remote environments, deploying the code, running the application and test cases, generating a report, and downloading log files.

Dfuntest is written in Java but the tested application may be written in any language since dfuntest tests the application by querying the application's external interface.
Having the testing library and the tests in a statically-typed language helps to avoid common bugs, which would be only visible at the runtime in a dynamically-typed language. 
Static typing also provides a way of documentation.

The ultimate goal of dfuntest is to become an equivalent of jUnit for distributed applications.
A tool that allows developers to focus on the logic in tests, rather than diluting in repetitive code and complex, partially-automated procedures.
A tool that facilitates switching parameters, settings, even platforms.
Finally, a tool that enables others to easily reproduce reported results and verify application's usefulness in their environment.

In this chapter dfuntest is presented.
First, we define a design pattern for testing distributed applications (Section~\ref{sec:dfuntest-design}).
Second, we present (Section~\ref{sec:dfunt-impl}) a concrete implementation of the proposed pattern --- the dfuntest framework.

Dfuntest is available as a separate project at \url{https://github.com/gregorias/dfuntest}.

\section{Dfuntest design}\label{sec:dfuntest-design}

\subsection{Abstracting tests of distributed applications}
One of the key features of testing frameworks are the structural abstractions of
the testing process. These abstractions are a result of a delicate equilibrium.
They must be general enough not to limit tester's expressiveness; but they must
be feasible to automate by the library code. A distributed testing framework
is no exception.

We will use the following vocabulary when describing the architecture. 
The framework runs on a single host called the \emph{testmaster}.
The application/system tested by the framework is the \emph{tested application}.
This application is composed of many \emph{instances};
each instance runs on an \emph{environment}---a remote host, or a separate directory on the testmaster.

We recognize the following phases when running a single test of a distributed
application:

\begin{enumerate}
\item \textbf{Test configuration} The tester decides which scenarios to run, on what
  hosts, and with which parameters.
\item \textbf{Environment preparation} The application and the test data are deployed on the target environments.
\item \textbf{Testing} The tested application runs according to a pre-defined \emph{scenario}; the test checks assertions on the application's state.
\item \textbf{Report generation} A report includes the test
  result and supplementary information for debugging, such as logs.
\item \textbf{Clean up} Generated artifacts (and perhaps also the environment) are deleted from remote hosts.
\end{enumerate}

A feature specific to distributed tests is the point of control of a test---that
describes how the testing scenario is executed. The point of control has the
following design axes:

\begin{description}
  \item{\textbf{Decentralization}} The degree by which the code describing the testing scenario is itself distributed.

    Some testing scenarios are inherently distributed, e.g., checking whether
    neighbors of a host respond to a periodic ping.  However, centralized
    control is more flexible, as it is easier to verify global properties of the
    application (e.g.: whether the induced graph is connected, see
    Section~\ref{sec:exampl-test-kademl}).  A scenario with distributed control
    can be centralized by exposing the requested behavior in an external
    interface of the tested application. In contrast, decentralization of a centralized
    test is more difficult.
    We refer to~\cite{ulr99,hie08,hie12} for further discussion.

    Centralized control introduces a single point of responsibility.
    This is not a true weakness, since it allows for more accurate detection and reasoning about errors.
    More importantly, a centralized control is less scalable.
    Lesser scalability might prevent the framework from effective testing of larger networks.

  \item{\textbf{Interactivity}} How much does the testing control interacts with
    the tested application over the course of the test.
    For example an interactive test may analyze workload and responsibilities of
    individual nodes in the tested application and try to put additional work to
    nodes whose failure may break the tested application.
\end{description}


\subsection{Architecture of Dfuntest}

\begin{figure}[tbp]
  \centering
  \def\svgwidth{\columnwidth}
  \scriptsize {
  \input{dfuntest_deployment.pdf_tex}
}
\caption{Deployment architecture}
\label{fig:4_deployment_architecture}.
\end{figure}

Dfuntest assumes a centralized control of a test. A single master
host --- the testmaster --- executes all phases of the test.
Tests scenarios can be highly interactive.
They are limited only by the interface provided by the tested application.
The deployment architecture of dfuntest testing process is shown on Figure \ref{fig:4_deployment_architecture}.
In the figure, grey boxes represent physical hosts.
White boxes represent dfuntest classes in testmaster and the tested application in remote nodes.
Arrows represent dependencies and communication channels between elements. 
\texttt{TestRunner} runs sequentially \texttt{TestScript}s, which in turn use proxy \texttt{App}s to run the test.
Instances of the tested application accept requests from the testmaster and use the Internet to communicate among themselves.

We describe the architecture of dfuntest by tracing the flow of a single test (as
described in the previous section); to motivate certain design decisions we
start our description from the \emph{testing} phase.

The testing phase consists of executing a certain scenario with actors
corresponding to applications running on (remote) environments.
This scenario is a \texttt{TestScript}; it is defined by the tester. The
scenario needs actors which are (remote) instances of the application --- or, more
accurately, proxies to external interfaces of the instances of the application that run on (remote) 
environments.
This proxy is represented by an \texttt{App} class.

To prepare the test environment on remote hosts, our framework uses standard OS
tools to copy, upload, or download files, traverse directories, run processes
etc.  Dfuntest abstracts from concrete implementations of these tools by a proxy
class \texttt{Environment}.
An abstract \texttt{Environment} enables dfuntest to abstract from where the
environment is located or how it is accessed.
This separation of \texttt{Environment} from \texttt{App} gives greater
flexibility and allows testing an application in various environments.
One frequent use-case is a test failing on a remote environment---by changing
the remote environment to a local one (i.e., the whole system runs as multiple
processes on a single host), debugging is faster and more accessible.

The process of deploying an instance of the tested application to an
environment (e.g., choosing which test data to put on which hosts)
depends strongly on the particular user application. This functionality is
described by the tester and encompassed in \texttt{EnvironmentPreparator}.

A test should result in an artifact documenting its results and logs to debug
possible failures. In dfuntest, the \texttt{TestScript} is responsible for this
functionality.

After tests are finished we may want to clean up environments and download test's artificats.
These are additional responsibilities of \texttt{EnvironmentPreparator}.

The class architecture of the process described above is shown on Figure
\ref{fig:4_class_architecture}.

\begin{figure}[tbp]
  \centering
  \def\svgwidth{\columnwidth}
  \scriptsize {
  \input{dfuntest_bw2.pdf_tex}
}
\caption{Class architecture}
\label{fig:4_class_architecture}
\end{figure}

\section{Dfuntest implementation}\label{sec:dfunt-impl}

In this section, we describe how dfuntest maps the abstractions sketched in the previous section to concrete code. Dfuntest defines a number of interfaces and provides reusable tools (such as concrete \texttt{Environment}s used for interacting with remote hosts) to stitch a coherent testing framework.

\paragraph{\texttt{Environment}} interface (Figure~\ref{fig:env_interface}) represent a proxy
object that performs typical shell operations on an environment.
Dfuntest provides two implementations of an \texttt{Environment}:
a \texttt{LocalEnvironment} and an \texttt{SSHEnvironment}.
In a local test, an application is deployed to multiple directories of the testmaster; the \texttt{LocalEnvironment} acts on the provided directory.
An \texttt{SSHEnvironment} connects to a remote host and translates method calls to SSH functions, e.g. \texttt{copy} to \texttt{scp}.

A tester may want to add new functions to \texttt{Environment}. For this reason, we defined other dfuntest interfaces as generics, taking a subclass of \texttt{Environment} as a parameter. 

\begin{figure}[tbp]
\begin{lstlisting}
public interface Environment {
  void copyFilesFromLocalDisk(Path srcPath, String destRelPath)
    throws IOException;
  void copyFilesToLocalDisk(String srcRelPath, Path destPath)
    throws IOException;
  String getHostname();
  int getId();
  String getName();
  Object getProperty(String key) throws NoSuchElementException;
  RemoteProcess runCommand(List<String> command)
    throws InterruptedException, IOException;
  RemoteProcess runCommandAsynchronously(List<String> command)
    throws IOException;
  void removeFile(String relPath) throws InterruptedException, IOException;
  void setProperty(String key, Object value);
}
\end{lstlisting}
\caption{\texttt{Environment} interface}
\label{fig:env_interface}
\end{figure}

\paragraph{\texttt{EnvironmentPreparator}} (Figure \ref{fig:envprepint}) defines the environmental dependencies between the application and its environment. The preparator prepares the environment, collects any output (including logs), and cleans the environment. The tester implements the preparator, as this process is specific to a concrete application.

Some applications depend on many external libraries, or datasets; copying these
files to remote hosts takes time.
To speed-up environment preparation for subsequent tests in a test suite, we
split the preparation process into two methods:
\texttt{prepare} assumes an empty environment, and thus copies all dependencies;
while \texttt{restore} assumes that all read-only files have been loaded.

\begin{figure}[tbp]
\begin{lstlisting}
public interface EnvironmentPreparator<EnvironmentT extends Environment> {
  // Prepare the environment from scratch.
  void prepare(Collection<EnvironmentT> envs) throws IOException;
  // Restore the environment after cleanOutput
  void restore(Collection<EnvironmentT> envs) throws IOException;
  // Download output and log files to local destPath
  void collectOutput(Collection<EnvironmentT> envs, Path destPath);
  // Remove all files generated by the application
  void cleanOutput(Collection<EnvironmentT> envs);
  // Restore the environment to clean state
  void cleanAll(Collection<EnvironmentT> envs);
}
\end{lstlisting}
\caption{\texttt{EnvironmentPreparator} interface}
\label{fig:envprepint}
\end{figure}

\paragraph{\texttt{App}}
The App interface is a proxy translating Java method calls to RPC invocations to a concrete instance of the tested application (running on a (remote) environment).
A tester should subclass \texttt{App} and add methods which correspond to the external RPC interface of their application.

\paragraph{\texttt{TestScript}}, the interface for testing scenarios, is the one
that will be subclassed the most and that is also the simplest. It implements
only one method \texttt{run} which executes the testing scenario, checks
assertions, and returns a report.

\paragraph{\texttt{EnvironmentFactory}, \texttt{AppFactory}} Since
\texttt{Environment} and \texttt{App} are meant to be subclassed, dfuntest uses
the Factory pattern to hide specific implementation from classes that do not require
it, like \texttt{TestRunner}.

\paragraph{\texttt{TestRunner}} A runner is an object which takes all previous
classes as dependencies, including a collection of \texttt{TestScript}s to run,
and runs the entire testing pipeline. It is a dfuntest equivalent of a
\texttt{Runner} in jUnit.
\texttt{TestRunner} uses the \texttt{EnvironmentFactory} to
create and prepare remote environments. Then, for each test, it uses the
\texttt{AppFactory} to create instances of \texttt{App}s (which in turn start
the remote instances of application). Once \texttt{App}s are created,
\texttt{TestRunner} runs \texttt{TestScript}s, collecting logs and cleaning
environments in-between. Finally it produces a report directory.

\section{Related work}
The reader may suspect by now that the author might suffer from the "Not Invented Here" syndrome \footnote{\url{https://en.wikipedia.org/wiki/Not\_invented\_here}} (the author certainly asks himself this question from time to time).
After all, secure DHT and distributed testing of peer-to-peer applications should be solved problems.
Although several frameworks for testing distributed applications have been proposed, we found none that would have dfuntest scope and we would be able to use.
In this section we review research articles as well as available software that falls into dfuntest's scope

\cite{ulr99}~proposes a decentralized testing architecture and presents a tool for distributed monitoring and assessment of test events. This tool does not facilitate deployment automation.
In~\cite{tsa03}, a test scenario defined in an XML file uses tested application's external SOAP interface. 
While dfuntest also uses external interface, we envision that this interface is enriched for particular tests (new methods  added); moreover, scenarios as Java methods enable greater expressiveness.
In~\cite{hug04}, the code of the tested application is modified using aspects (thus, the framework tests only Java code). 
Given examples focus on monitoring rather than testing---tests can verify how many nodes are, e.g., executing a method.
\cite{de10}~uses annotations, which again limits the applicability of the framework to Java applications. 
The scenarios are defined in a pseudo-language that, compared to dfuntest, might increase readability, but also reduce expressiveness.
The framework is more distributed compared to dfuntest, as proxy objects (similar to our \texttt{App}) run on remote hosts. 
Remote proxies reduce the need for an external interface; 
however, dfuntest centralization helps to check assertions on whole state of the system.
\cite{tor10}~focuses on methods of isolating submodules by emulating some of the components---dfuntest tests the whole distributed application. 

To our best knowledge, frameworks described above are not publicly available. In addition to described differences, they do not abstract the remote environment (dfuntest's \texttt{Environment} and \texttt{EnvironmentPreparator}), thus they do not facilitate deployment, nor porting tests between user credentials or testing infrastructures.

We continue with the available software for distributed testing.
The Software Testing Automation Framework (STAF) \footnote{\url{http://staf.sourceforge.net}} is an open source
project that creates cross-platform, distributed software test environments. It
uses services to provide an uniform interface to environment's resources, such
as file system, process management etc. From an architectural point of view its
services correspond to \texttt{Environment} abstraction layer in dfuntest.
If needed, it is possible to extend \texttt{Environment} interface with STAF facilities and provide its features.

SmartBear TestComplete \footnote{\url{http://smartbear.com/product/testcomplete/overview}} proprietary product has distributed testing functionality.
The framework allows definition of arbitrary environments and runs jobs sequentially.
SmartBear does not provide any particular mechanism for running a testing scenario or generating a testing report.
Additionally the software requires that the environment has the TestComplete software installed and running and the application uses TestComplete bindings.

Robot Framework \footnote{\url{http://robotframework.org}} is a generic test automation framework for acceptance testing and acceptance test-driven development.
Users can define their testing scenarios in a high-level language resembling natural language.
Robot Framework then automates running and generating a testing report.
Robot does not provide any mechanisms for distributed test control and flexible distributed environment preparation.

Neither of those libraries covers the entire scope of dfuntest framework. What
all of them lack is the ability to define complex testing scenarios
programatically, which is provided by script and app abstraction layers.
